{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC2QnhmKxpq1"
   },
   "source": [
    "**Please set up your credentials JSON as GCP_CREDENTIALS secrets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "# Load GCP credentials path and bucket URL from environment variables\n",
    "credentials_path = os.getenv(\"GCP_CREDENTIALS_PATH\")\n",
    "bucket_url = os.getenv(\"BUCKET_URL\")\n",
    "\n",
    "try:\n",
    "    with open(credentials_path, \"r\") as f:\n",
    "        credentials_json = f.read()\n",
    "    os.environ[\"DESTINATION__CREDENTIALS\"] = credentials_json\n",
    "    os.environ[\"BUCKET_URL\"] = bucket_url\n",
    "except FileNotFoundError:\n",
    "    print(f\"Credentials file not found: {credentials_path}. Skipping credential setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dlt.destinations import filesystem\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'de-zoomcamp-taxi-data-2024' already exists.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "# After loading creds_dict, extract project_id from creds_dict\n",
    "with open(os.getenv(\"GCP_CREDENTIALS_PATH\"), \"r\") as f:\n",
    "    creds_dict = json.load(f)\n",
    "\n",
    "client = storage.Client.from_service_account_info(creds_dict)\n",
    "\n",
    "# Extract bucket name from BUCKET_URL\n",
    "bucket_name = bucket_url.replace(\"gs://\", \"\").split(\"/\")[0]\n",
    "\n",
    "# Create bucket if it does not exist\n",
    "if not client.lookup_bucket(bucket_name):\n",
    "    try:\n",
    "        bucket = client.create_bucket(bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Bucket '{bucket_name}' does not exist and cannot be created due to billing issues. Skipping bucket creation.\")\n",
    "else:\n",
    "    print(f\"Bucket '{bucket_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesting parquet files to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for month: 1\n",
      "Downloading data for month: 2\n",
      "Downloading data for month: 3\n",
      "Downloading data for month: 4\n",
      "Downloading data for month: 5\n",
      "Downloading data for month: 6\n",
      "Pipeline rides_pipeline load step completed in 35.48 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset rides_dataset\n",
      "The filesystem destination used gs://de-zoomcamp-taxi-data-2024 location to store data\n",
      "Load package 1770677102.364394 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Define a dlt source to download and process Parquet files as resources\n",
    "@dlt.source(name=\"rides\")\n",
    "def download_parquet():\n",
    "    prefix = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata\"\n",
    "    for month in range(1, 7):\n",
    "        print(f\"Downloading data for month: {month}\")\n",
    "        file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
    "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        df = pd.read_parquet(BytesIO(response.content))\n",
    "\n",
    "        # Return the dataframe as a dlt resource for ingestion\n",
    "        yield dlt.resource(df, name=file_name)\n",
    "\n",
    "\n",
    "# Initialize the pipeline\n",
    "gcs_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rides_pipeline\",\n",
    "    destination=filesystem(layout=\"{schema_name}/{table_name}.{ext}\"),\n",
    "    dataset_name=\"rides_dataset\",\n",
    ")\n",
    "\n",
    "# Run the pipeline to load Parquet data into DuckDB\n",
    "load_info = gcs_pipeline.run(download_parquet(), loader_file_format=\"parquet\")\n",
    "\n",
    "# Print the results\n",
    "print(load_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an External Table in BigQuery referencing the Parquet files in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset rides_dataset exists or you have access.\n",
      "External table created: tokyo-epoch-486603-u6:rides_dataset.yellow_tripdata_external\n"
     ]
    }
   ],
   "source": [
    "# Create an external table in BigQuery referencing the Parquet files in GCS\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "# Use the already loaded project_id variable\n",
    "project_id = creds_dict.get(\"project_id\")\n",
    "dataset_id = \"rides_dataset\"\n",
    "external_table_id = \"yellow_tripdata_external\"\n",
    "\n",
    "# Parquet files location in GCS (adjust path if needed)\n",
    "parquet_uri = f\"{bucket_url}/rides_dataset/*.parquet\"\n",
    "\n",
    "bq_client = bigquery.Client.from_service_account_info(creds_dict, project=project_id)\n",
    "\n",
    "# Create dataset if not exists\n",
    "dataset_ref = bigquery.Dataset(f\"{project_id}.{dataset_id}\")\n",
    "try:\n",
    "    bq_client.get_dataset(dataset_ref)\n",
    "    print(f\"Dataset {dataset_id} exists or you have access.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not access dataset {dataset_id}. This may be due to insufficient permissions, but if you created it manually and have access in the UI, you can ignore this warning. Error: {e}\")\n",
    "\n",
    "# Proceed to create the external table regardless of the above error\n",
    "external_config = bigquery.ExternalConfig(\"PARQUET\")\n",
    "external_config.source_uris = [parquet_uri]\n",
    "\n",
    "table_ref = f\"{project_id}.{dataset_id}.{external_table_id}\"\n",
    "external_table = bigquery.Table(table_ref)\n",
    "external_table.external_data_configuration = external_config\n",
    "\n",
    "try:\n",
    "    external_table = bq_client.create_table(external_table, exists_ok=True)\n",
    "    print(f\"External table created: {external_table.full_table_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Could not create external table. You do not have bigquery.tables.create permission on dataset {dataset_id}. Details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a regular table in BigQuery from the external table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materialized table created: tokyo-epoch-486603-u6.rides_dataset.yellow_tripdata\n"
     ]
    }
   ],
   "source": [
    "# Create a regular (materialized) table in BigQuery from the external table\n",
    "materialized_table_id = \"yellow_tripdata\"\n",
    "materialized_table_ref = f\"{project_id}.{dataset_id}.{materialized_table_id}\"\n",
    "\n",
    "query = f'''\n",
    "CREATE OR REPLACE TABLE `{materialized_table_ref}` AS\n",
    "SELECT * FROM `{project_id}.{dataset_id}.{external_table_id}`\n",
    "'''\n",
    "\n",
    "job = bq_client.query(query)\n",
    "job.result()  # Wait for job to finish\n",
    "print(f\"Materialized table created: {materialized_table_ref}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['vendor_id', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecode_id', 'store_and_fwd_flag', 'pu_location_id', 'do_location_id', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n"
     ]
    }
   ],
   "source": [
    "# Show column names of the materialized BigQuery table\n",
    "table = bq_client.get_table(materialized_table_ref)\n",
    "print(\"Column names:\", [field.name for field in table.schema])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Data read estimation\n",
    "    Write a query to count the distinct number of PULocationIDs for the entire dataset on both the tables.\n",
    "    What is the estimated amount of data that will be read when this query is executed on the External Table and the Table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated bytes read from external table: 0\n",
      "Estimated bytes read from materialized table: 162,656,744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct pu_location_ids (external table): 262\n",
      "Distinct pu_location_ids (materialized table): 262\n"
     ]
    }
   ],
   "source": [
    "# Count distinct pu_location_ids and estimate bytes processed for both tables\n",
    "from google.cloud.bigquery import QueryJobConfig, QueryJob\n",
    "\n",
    "def estimate_query_bytes(query):\n",
    "    job_config = QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    query_job = bq_client.query(query, job_config=job_config)\n",
    "    return query_job.total_bytes_processed\n",
    "\n",
    "external_table_query = f'''\n",
    "SELECT COUNT(DISTINCT pu_location_id) AS distinct_pu FROM `{project_id}.{dataset_id}.{external_table_id}`\n",
    "'''\n",
    "materialized_table_query = f'''\n",
    "SELECT COUNT(DISTINCT pu_location_id) AS distinct_pu FROM `{project_id}.{dataset_id}.{materialized_table_id}`\n",
    "'''\n",
    "\n",
    "# Estimate data read for external table\n",
    "external_bytes = estimate_query_bytes(external_table_query)\n",
    "print(f\"Estimated bytes read from external table: {external_bytes:,}\")\n",
    "\n",
    "# Estimate data read for materialized table\n",
    "materialized_bytes = estimate_query_bytes(materialized_table_query)\n",
    "print(f\"Estimated bytes read from materialized table: {materialized_bytes:,}\")\n",
    "\n",
    "# Optionally, run the queries to get the actual counts\n",
    "external_count = bq_client.query(external_table_query).result().to_dataframe().iloc[0,0]\n",
    "materialized_count = bq_client.query(materialized_table_query).result().to_dataframe().iloc[0,0]\n",
    "print(f\"Distinct pu_location_ids (external table): {external_count}\")\n",
    "print(f\"Distinct pu_location_ids (materialized table): {materialized_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingesting data to Local Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 23:11:47,296|[WARNING]|1457|140261840968640|dlt|pipeline.py|_state_to_props:1731|The destination dlt.destinations.filesystem:None in state differs from destination dlt.destinations.duckdb:duckdb in pipeline and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for month: 1\n",
      "Downloading data for month: 2\n",
      "Downloading data for month: 3\n",
      "Downloading data for month: 4\n",
      "Downloading data for month: 5\n",
      "Downloading data for month: 6\n",
      "Pipeline rides_pipeline load step completed in 3.47 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset rides_dataset\n",
      "The duckdb destination used duckdb:////app/notebooks/data/rides_pipeline.db location to store data\n",
      "Load package 1770678707.380898 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Define a dlt resource to download and process Parquet files as single table\n",
    "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
    "def download_parquet():\n",
    "    prefix = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata'\n",
    "    for month in range(1, 7):\n",
    "        print(f\"Downloading data for month: {month}\")\n",
    "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
    "        response = requests.get(url)\n",
    "        df = pd.read_parquet(BytesIO(response.content))\n",
    "        yield df\n",
    "\n",
    "# Use .env values for DuckDB path and dataset\n",
    "import os\n",
    "os.environ.pop('DUCKDB_PATH', None)  # Remove existing DUCKDB_PATH if set\n",
    "duckdb_path = os.getenv('DUCKDB_PATH', 'data/rides_pipeline.db')\n",
    "duckdb_dataset = os.getenv('DUCKDB_DATASET', 'rides_dataset')\n",
    "\n",
    "# Unset GCP-related env vars to avoid contamination\n",
    "# os.environ.pop('GCP_CREDENTIALS_PATH', None)\n",
    "# os.environ.pop('BUCKET_URL', None)\n",
    "# os.environ.pop('DESTINATION__CREDENTIALS', None)\n",
    "\n",
    "# Set DuckDB path for dlt\n",
    "os.environ['DUCKDB_DATABASE'] = duckdb_path\n",
    "\n",
    "# Initialize the pipeline\n",
    "duck_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rides_pipeline\",\n",
    "    destination=dlt.destinations.duckdb(duckdb_path),  # must be the string 'duckdb', not a path\n",
    "    dataset_name=duckdb_dataset,\n",
    "    # dlt will use duckdb_path automatically if set in env, but you can also set via env or config\n",
    ")\n",
    "\n",
    "# Run the pipeline to load Parquet data into DuckDB\n",
    "info = duck_pipeline.run(download_parquet)\n",
    "\n",
    "# Print the results\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         database         schema                 name  \\\n",
      "0  rides_pipeline  rides_dataset           _dlt_loads   \n",
      "1  rides_pipeline  rides_dataset  _dlt_pipeline_state   \n",
      "2  rides_pipeline  rides_dataset         _dlt_version   \n",
      "3  rides_pipeline  rides_dataset                rides   \n",
      "\n",
      "                                        column_names  \\\n",
      "0  [load_id, schema_name, status, inserted_at, sc...   \n",
      "1  [version, engine_version, pipeline_name, state...   \n",
      "2  [version, engine_version, inserted_at, schema_...   \n",
      "3  [vendor_id, tpep_pickup_datetime, tpep_dropoff...   \n",
      "\n",
      "                                        column_types  temporary  \n",
      "0  [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME...      False  \n",
      "1  [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP W...      False  \n",
      "2  [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VAR...      False  \n",
      "3  [INTEGER, TIMESTAMP WITH TIME ZONE, TIMESTAMP ...      False  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{duck_pipeline.pipeline_name}.duckdb\")\n",
    "\n",
    "# Set search path to the dataset\n",
    "conn.sql(f\"SET search_path = '{duck_pipeline.dataset_name}'\")\n",
    "\n",
    "# Describe the dataset to see loaded tables\n",
    "res = conn.sql(\"DESCRIBE\").df()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. Counting records\n",
    "\n",
    "    What is count of records for the 2024 Yellow Taxi Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count(1)\n",
      "0  20332093\n"
     ]
    }
   ],
   "source": [
    "# provide a resource name to query a table of that name\n",
    "with duck_pipeline.sql_client() as client:\n",
    "    with client.execute_query(f\"SELECT count(1) FROM rides\") as cursor:\n",
    "        data = cursor.df()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Data read estimation\n",
    "\n",
    "    Write a query to count the distinct number of PULocationIDs for the entire dataset on both the tables.\n",
    "    What is the estimated amount of data that will be read when this query is executed on the External Table and the Table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB Table distinct pu_location_id:    distinct_pu\n",
      "0          262\n"
     ]
    }
   ],
   "source": [
    "# Count distinct pu_location_id in DuckDB table\n",
    "with duck_pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT COUNT(DISTINCT pu_location_id) AS distinct_pu FROM rides\") as cursor:\n",
    "        duckdb_distinct = cursor.df()\n",
    "print(\"DuckDB Table distinct pu_location_id:\", duckdb_distinct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Understanding columnar storage\n",
    "\n",
    "    Write a query to retrieve the PULocationID from the table (not the external table) in BigQuery. Now write a query to retrieve the PULocationID and DOLocationID on the same table.\n",
    "\n",
    "    Why are the estimated number of Bytes different?\n",
    "\n",
    "        BigQuery is a columnar database, and it only scans the specific columns requested in the query. Querying two columns (PULocationID, DOLocationID) requires reading more data than querying one column (PULocationID), leading to a higher estimated number of bytes processed.\n",
    "        BigQuery duplicates data across multiple storage partitions, so selecting two columns instead of one requires scanning the table twice, doubling the estimated bytes processed.\n",
    "        BigQuery automatically caches the first queried column, so adding a second column increases processing time but does not affect the estimated bytes scanned.\n",
    "        When selecting multiple columns, BigQuery performs an implicit join operation between them, increasing the estimated bytes processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated bytes processed for one column (pu_location_id): 162,656,744\n",
      "Estimated bytes processed for two columns (pu_location_id, do_location_id): 325,313,488\n"
     ]
    }
   ],
   "source": [
    "# Estimate bytes processed for selecting one vs two columns from materialized table\n",
    "def estimate_query_bytes(query):\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    query_job = bq_client.query(query, job_config=job_config)\n",
    "    return query_job.total_bytes_processed\n",
    "\n",
    "single_col_query = f\"SELECT pu_location_id FROM `{project_id}.{dataset_id}.{materialized_table_id}`\"\n",
    "two_col_query = f\"SELECT pu_location_id, do_location_id FROM `{project_id}.{dataset_id}.{materialized_table_id}`\"\n",
    "\n",
    "single_col_bytes = estimate_query_bytes(single_col_query)\n",
    "two_col_bytes = estimate_query_bytes(two_col_query)\n",
    "\n",
    "print(f\"Estimated bytes processed for one column (pu_location_id): {single_col_bytes:,}\")\n",
    "print(f\"Estimated bytes processed for two columns (pu_location_id, do_location_id): {two_col_bytes:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. Counting zero fare trips\n",
    "\n",
    "How many records have a fare_amount of 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with fare_amount = 0: 8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Count records with fare_amount = 0 in BigQuery materialized table\n",
    "zero_fare_query = f\"SELECT COUNT(*) AS zero_fare_count FROM `{project_id}.{dataset_id}.{materialized_table_id}` WHERE fare_amount = 0\"\n",
    "zero_fare_count = bq_client.query(zero_fare_query).result().to_dataframe().iloc[0,0]\n",
    "print(f\"Number of records with fare_amount = 0: {zero_fare_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. Partitioning and clustering\n",
    "\n",
    "What is the best strategy to make an optimized table in Big Query if your query will always filter based on tpep_dropoff_datetime and order the results by VendorID (Create a new table with this strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized table created: tokyo-epoch-486603-u6.rides_dataset.yellow_tripdata_partitioned_clustered\n"
     ]
    }
   ],
   "source": [
    "# Create an optimized table partitioned by tpep_dropoff_datetime and clustered by VendorID\n",
    "optimized_table_id = \"yellow_tripdata_partitioned_clustered\"\n",
    "optimized_table_ref = f\"{project_id}.{dataset_id}.{optimized_table_id}\"\n",
    "\n",
    "create_optimized_query = f'''\n",
    "CREATE OR REPLACE TABLE `{optimized_table_ref}`\n",
    "PARTITION BY DATE(tpep_dropoff_datetime)\n",
    "CLUSTER BY vendor_id AS\n",
    "SELECT * FROM `{project_id}.{dataset_id}.{materialized_table_id}`\n",
    "'''\n",
    "\n",
    "job = bq_client.query(create_optimized_query)\n",
    "job.result()  # Wait for job to finish\n",
    "print(f\"Optimized table created: {optimized_table_ref}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. Partition benefits\n",
    "\n",
    "Write a query to retrieve the distinct VendorIDs between tpep_dropoff_datetime 2024-03-01 and 2024-03-15 (inclusive)\n",
    "\n",
    "Use the materialized table you created earlier in your from clause and note the estimated bytes. Now change the table in the from clause to the partitioned table you created for question 5 and note the estimated bytes processed. What are these values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated bytes processed (materialized table): 325,313,488\n",
      "Estimated bytes processed (partitioned table): 28,141,776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct VendorIDs (materialized table): [6, 1, 2]\n",
      "Distinct VendorIDs (partitioned table): [1, 6, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.13/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Partition benefits - Estimate bytes for materialized and partitioned tables\n",
    "start_date = '2024-03-01'\n",
    "end_date = '2024-03-15'\n",
    "\n",
    "materialized_query = f'''\n",
    "SELECT DISTINCT vendor_id FROM `{project_id}.{dataset_id}.{materialized_table_id}`\n",
    "WHERE tpep_dropoff_datetime BETWEEN '{start_date}' AND '{end_date}'\n",
    "'''\n",
    "\n",
    "partitioned_table_id = \"yellow_tripdata_partitioned_clustered\"\n",
    "partitioned_query = f'''\n",
    "SELECT DISTINCT vendor_id FROM `{project_id}.{dataset_id}.{partitioned_table_id}`\n",
    "WHERE tpep_dropoff_datetime BETWEEN '{start_date}' AND '{end_date}'\n",
    "'''\n",
    "\n",
    "# Estimate bytes processed for both queries\n",
    "materialized_bytes = estimate_query_bytes(materialized_query)\n",
    "partitioned_bytes = estimate_query_bytes(partitioned_query)\n",
    "\n",
    "print(f\"Estimated bytes processed (materialized table): {materialized_bytes:,}\")\n",
    "print(f\"Estimated bytes processed (partitioned table): {partitioned_bytes:,}\")\n",
    "\n",
    "# Optionally, run both queries and print distinct vendor_ids\n",
    "materialized_vendor_ids = bq_client.query(materialized_query).result().to_dataframe()['vendor_id'].tolist()\n",
    "partitioned_vendor_ids = bq_client.query(partitioned_query).result().to_dataframe()['vendor_id'].tolist()\n",
    "print(f\"Distinct VendorIDs (materialized table): {materialized_vendor_ids}\")\n",
    "print(f\"Distinct VendorIDs (partitioned table): {partitioned_vendor_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7. External table storage\n",
    "\n",
    "Where is the data stored in the External Table you created?\n",
    "    GCP Bucket\n",
    "\n",
    "## Question 8. Clustering best practices\n",
    "\n",
    "It is best practice in Big Query to always cluster your data:\n",
    "\n",
    "    FALSE.\n",
    "    Clustering in BigQuery is not always best practice. Itâ€™s useful when queries frequently filter or group by clustered columns, reducing data scanned and improving performance. Otherwise, clustering adds overhead and may not help. Use clustering based on query patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9. Understanding table scans\n",
    "\n",
    "No Points: Write a SELECT count(*) query FROM the materialized table you created. How many bytes does it estimate will be read? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materialized table row count: 20332093\n",
      "Estimated bytes processed for SELECT count(*): 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: Ensure materialized table is not empty\n",
    "row_count_query = f\"SELECT COUNT(*) AS row_count FROM `{project_id}.{dataset_id}.{materialized_table_id}`\"\n",
    "row_count = bq_client.query(row_count_query).result().to_dataframe().iloc[0,0]\n",
    "print(f\"Materialized table row count: {row_count}\")\n",
    "if row_count == 0:\n",
    "    print(\"Warning: Materialized table is empty. Estimated bytes processed will be 0.\")\n",
    "# Estimate bytes processed for SELECT count(*) FROM materialized table\n",
    "count_query = f\"SELECT count(*) FROM `{project_id}.{dataset_id}.{materialized_table_id}`\"\n",
    "count_bytes = estimate_query_bytes(count_query)\n",
    "print(f\"Estimated bytes processed for SELECT count(*): {count_bytes:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated bytes processed for `SELECT count(*)` is often equal to the total size of all columns in the table, because BigQuery must scan every row to count them. However, for some tables, BigQuery can use metadata optimizations to return the row count without scanning any data, resulting in zero bytes processed. In a columnar database, only the requested columns are scanned unless metadata is available for optimizations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
